---
title: "Data Management and Analysis Workshop: Part 1"
format: pdf
editor: visual
---

# Dataset assembly

We will walk through some basic data cleaning, analysis, and visualization tasks using per pupil expenditures as a running example. The repository contains all the data and code we need, so you can just follow along! This notebook introduces data cleaning and dataset assembly.

## Load libraries

The tidyverse is a popular collection of R libraries for data science. They provide functions and data structures that extend what is available in base R, i.e., out-of-the-box. Most data wrangling tasks use dplyr and tidyr packages, and most data visualization is with ggplot2. For details, features, and examples not covered in this notebook, please see the documentation [here](https://www.tidyverse.org/packages/).

```{r}
# Install required packages.
req <- c("tidyverse", "openxlsx")
new <- req[!(req %in% installed.packages()[, "Package"])]
if (length(new)) install.packages(new)

# Load required packages.
library(dplyr)
library(tidyr)
library(openxlsx)
```

## Load data

Keep a simple inventory of the files that the program reads and writes at the top of the script. This will orient anyone who reads the code to its requirements and results. Everything the program needs should be contained in the repository.

Set your working directory to wherever you downloaded this repository. This tells R the folder in which to look for the data and save our results. Please see the R Studio documentation [here](https://support.posit.co/hc/en-us/articles/200711843-Working-Directories-and-Workspaces-in-the-RStudio-IDE) for how to set the working directory.

```{r}
# Identify inputs and outputs.
PWD <- getwd()
FIS <- file.path(PWD, "in", "sdf21_1a.txt")
MEM <- file.path(PWD, "in", "ccd_lea_052_2021_l_1a_080621.csv")
DTA <- file.path(PWD, "out", "data.rda")
XLS <- file.path(PWD, "out", "data.xlsx")
```

```{r}
# Load and peek at the fiscal data.
# Need to use the function that corresponds to the file type, e.g., txt.
fis <- read.delim(FIS)
head(fis)
```

```{r}
# These are some helpful functions for accessing information about the dataset.
print(paste("Number of rows:", nrow(fis)))
print(paste("Number of columns:", ncol(fis)))
```

## Check unique identifier

We think the fiscal data contain one observation for each school district. This is an important assumption that would compromise our analysis and break our code were it to fail. For example, there may be a more complicated nesting structure or duplicates that we'd need to handle first. Let's test this.

```{r}
# Ensure data are at the school district level.
leaid <- unique(fis$LEAID)
stopifnot(nrow(fis) == length(leaid))
```

**Technical note.** The code makes extensive use of the `stopifnot()` function as a defensive programming technique. It takes an expression that must evaluate to `TRUE`, and otherwise throws an error and stops the program. This helps document and verify assumptions of the data.

## Handle missing data

The columns `MEMBERSCH` and `TOTALEXP` describe total number of students and total expenditures, respectively. We can see that they contain some negative values, which is impossible! Reading the documentation, we learn that these values mark different kinds of missingness. We need to recode these as `NA` values to correctly analyze the data.

```{r}
# Summarize students and expenditures.
summary(fis[, c("MEMBERSCH", "TOTALEXP")])
```

```{r}
# Tabulate negative values in students.
table(fis$MEMBERSCH[fis$MEMBERSCH < 0])
```

```{r}
# Tabulate negative values in expenditures.
table(fis$TOTALEXP[fis$TOTALEXP < 0])
```

```{r}
# Recode missing values to NA.
fis <- mutate(fis, across(c("MEMBERSCH", "TOTALEXP"), ~ replace(.x, .x < 0, NA)))
```

```{r}
# Summarize students and expenditures again.
summary(fis[, c("MEMBERSCH", "TOTALEXP")])
```

**Technical note.** We used base R grammar to filter for missing values and tidyverse grammar to replace them. Both are correct, however sometimes one is more concise than the other. Like with any writing, aim for clear and simple code.

## Generate new measures

Now that we have a numerator and denominator, we can calculate a rate. In statistics, we call this defining a variable. But in the tidyverse, we call this creating a column. Note that we have some school districts with zero students, so we will use a conditional statement to generate these values where we can.

```{r}
# Calculate per pupil expenditure.
fis <- mutate(
  fis,
  ppe = case_when(
    MEMBERSCH > 0 ~ TOTALEXP / MEMBERSCH,
    .default = NA
  )
)
```

```{r}
# Summarize per pupil expenditure.
summary(fis$ppe)
```

## Transform data

We're also interested in how per pupil expenditure varies by majority race, but that information is not in the fiscal data. For that, we'll need to get the membership data.

```{r}
# Load and peek at the membership data.
# Need to use the function that corresponds to the file type, e.g., csv.
mem <- read.csv(MEM)
head(mem)
```

```{r}
# Note data are not at the school district level!
leaid <- unique(mem$LEAID)
stopifnot(nrow(mem) > length(leaid))
```

Inspecting the data and reading the documentation, we learn that the data contain multiple observations per school district to disaggregate membership by grade, race, and gender. We want to aggregate and reshape the data to the school district level with additional columns for total membership by race.

```{r}
# Write a function to check uniqueness of identifiers.
# Use this in a chain of tidyverse functions to check your work.
isid <- function(.data, ...) {
  if(any(duplicated(dplyr::select(.data, ...)))) {
    stop("indexers do not uniquely identify the observations")
  }
  return(.data)
}
```

```{r}
# Aggregate and reshape to the school district level.
lea <- mem |>
  # Select observations for the race-gender level.
  filter(TOTAL_INDICATOR == "Derived - Subtotal by Race/Ethnicity and Sex minus Adult Education Count") |>
  # Consolidate race/ethnicity categories with shorthand labels.
  mutate(race = case_match(
    RACE_ETHNICITY,
    "Asian"                                         ~ "asian",
    "Black or African American"                     ~ "black",
    "Hispanic/Latino"                               ~ "hisp",
    "White"                                         ~ "white",
    c("American Indian or Alaska Native",
      "Native Hawaiian or Other Pacific Islander",
      "Two or more races",
      "Not Specified")                              ~ "other"
  )) |>
  # Sum membership by race and overall within district.
  group_by(LEAID, race) |>
  summarize(n = sum(STUDENT_COUNT, na.rm = TRUE)) |>
  isid(LEAID, race) |>
  mutate(tot = sum(n)) |>
  ungroup() |>
  # Reshape to district level.
  pivot_wider(names_from = race, values_from = n) |>
  isid(LEAID) |>
  # Calculate group shares.
  rowwise() |>
  mutate(across(
    c(asian, black, hisp, white, other),
    ~ .x / tot * 100,
    .names = "pct_{.col}"
  )) |>
  # Create indicators on majority status.
  mutate(across(
    c(asian, black, hisp, white, other),
    ~ if_else(tot == 0, FALSE, .x / tot > 0.5),
    .names = "maj_{.col}"
    )
  ) |>
  ungroup() |>
  # Identify majority group.
  mutate(maj_group = case_when(
    maj_asian ~ "asian",
    maj_black ~ "black",
    maj_hisp  ~ "hisp",
    maj_white ~ "white",
    maj_other ~ "other",
    tot > 0   ~ "none"
  ))
```

**Technical note.** The last example uses the pipe operator `|>` to pass (a) the output of the previous function to (b) the input of the next function. This idiom allows chaining of discrete operations into an easily readable sequence of operations.

## Merge data

With both datasets cleaned, we can bring them together to assemble our analysis file. Specifically, we will join the columns by matching their rows with the school district identifier. We can say that the dataframes have a one-to-one relationship because each row in one will match to at most one row in the other.

```{r}
# Merge fiscal and membership data.
dta <- fis |>
  # Numericize school district identifier to enable merge.
  mutate(LEAID = as.numeric(LEAID)) |>
  # Merge datasets and keep matches.
  inner_join(lea, by = "LEAID", relationship = "one-to-one") |>
  # Keep non-missing values.
  filter(!is.na(ppe) & !is.na(maj_group))
```

Note that the inner join only keeps school districts that exist in both dataframes. We also drop school districts with missing values. We need to check how much of the sample we lose due to these decisions.

```{r}
# Check number of districts dropped in merge.
all_leaid <- unique(c(as.numeric(fis$LEAID), mem$LEAID))
pct_retain <- nrow(dta) / length(all_leaid) * 100
print(paste("Percent of districts retained:", sprintf("%3.2f", pct_retain)))
```

This seems like a lot. In a real project, we would go back to see how these school districts are distinct from the sample retained and whether they belong. If they do, we would need to find a way to recover the missing data or understand how our analysis could be biased.

## Save file

As a last step, keep just the data elements required and rename columns for readability.

```{r}
# Clean up.
dta <- dta |>
  # Choose one column for total district enrollment.
  select(-tot) |>
  rename(tot = MEMBERSCH) |>
  # Rename columns.
  rename(exp = TOTALEXP) |>
  rename_with(tolower, everything()) |>
  # Keep and reorder required columns.
  select(leaid, name, stname, tot, exp, ppe, starts_with("pct_"), maj_group)
```

With data cleaning complete, save the analysis file for use in the analysis program. Come back to the cleaning program to implement major revisions that the analysis requires. Aim to keep the workflow simple so that anyone who looks through the repository can follow along.

```{r}
# Save dataset to load into another R program.
# This puts the dataframe into the same namespace.
save(dta, file = DTA)

# Save to Excel workbook.
write.xlsx(dta, XLS)
```
